<!doctype html>
<html>
    <head>
        <title>Ed Space - Worknotes - CUDA</title>
        <link href="/css/general.css" rel="stylesheet">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
        <link href="/css/fortran-syntax.css" rel="stylesheet">
    </head>
    <body>
    
        <!-- Top navigation bar -->
        <nav class="navbar navbar-default sticky-top">
            <div class="container-fluid">
                <ul class="nav navbar-nav">
                    <li class="nav-item"><a href="/index.html">Home</a></li>
                    <li class="nav-item"><a href="/about.html">About Me</a></li>
                    <li class="nav-item active"><a href="/worknotes/index.html">Work Notes</a></li>
                    <li class="nav-item"><a href="/trips/index.html">Trips</a></li>
                    <li class="nav-item"><a href="/apps/index.html">Web Tools</a></li>
                  </ul>
            </div>
        </nav>
        
        <div class="normalwidth">
            <h1>testAsync</h1>
            <p>Program to demonstrate the run-time improvements when performing asynchronous memory transfers and execution.</p>
            <p>In the book, they show three different batching approaches, which yield different results based on the GPU. On my NVIDIA 1650 (Turing architecture), I see V1 producing best results <em>most</em> of the time. On my workplace's P100 (Keplar), A30 (Ampere), and A100 (Ampere) GPUs, the lowest run times are reliably obtained with V2 (but only about 1% difference).</p>
            <p><a href="testAsync.cuf" download="testAsync.cuf">Download link</a></p>
            <pre><code><span class="fcomment">! This code demonstrates strategies hiding data transfers via</span>
<span class="fcomment">! asynchronous data copies in multiple streams</span>
<span class="fstd">module</span> kernels_m
<span class="fstd">contains</span>

    <span class="fnv">attributes</span>(<span class="fnv">global</span>) <span class="fstd">subroutine</span> kernel(a, offset)
        <span class="fstd">implicit</span> <span class="fstd">none</span>
        <span class="fstd">real</span>:: a(*)
        <span class="fstd">integer</span>, <span class="fstd">value</span>:: offset
        <span class="fstd">integer</span>:: i
        <span class="fstd">real</span>:: c, s, x
    
        i = offset + <span class="fnv">threadIdx</span>%x + (<span class="fnv">blockIdx</span>%x-1)* <span class="fnv">blockDim</span>%x
        x = i; s = <span class="fintrinsic">sin</span>(x); c = <span class="fintrinsic">cos</span>(x)
        a(i) = a(i) + sqrt(s**2+c**2)
    <span class="fstd">end</span> <span class="fstd">subroutine</span> kernel

<span class="fstd">end</span> <span class="fstd">module</span> kernels_m

<span class="fstd">program</span> testAsync

    <span class="fstd">use</span> <span class="fnv">cudafor</span>
    <span class="fstd">use</span> kernels_m

    <span class="fstd">implicit</span> <span class="fstd">none</span>
    <span class="fstd">integer</span>, <span class="fstd">parameter</span>:: blockSize = 256, nStreams = 4
    <span class="fstd">integer</span>, <span class="fstd">parameter</span>:: n = 4*1024* blockSize*nStreams
    <span class="fstd">real</span>, <span class="fnv">pinned</span>, <span class="fstd">allocatable</span>:: a(:)
    <span class="fstd">real</span>, <span class="fnv">device</span>:: a_d(n)
    <span class="fstd">integer</span>(<span class="fstd">kind</span>=<span class="fnv">cuda_stream_kind</span>):: stream(nStreams)
    <span class="fstd">type</span>(<span class="fnv">cudaEvent</span>) :: startEvent, stopEvent, dummyEvent
    <span class="fstd">real</span>:: time
    <span class="fstd">integer</span>:: i, istat, offset, streamSize = n/nStreams
    <span class="fstd">logical</span>:: pinnedFlag
    <span class="fstd">type</span>(<span class="fnv">cudaDeviceProp</span>):: prop

    istat = <span class="fnv">cudaGetDeviceProperties</span>(prop , 0)
    <span class="fstd">write</span>(*,<span class="fstring">"(<span class="fstring">' Device: '</span>, a,/)"</span>) <span class="fintrinsic">trim</span>(prop%name)

    <span class="fcomment">! allocate pinned host memory</span>
    <span class="fstd">allocate</span>(a(n), <span class="fstd">STAT</span>=istat , <span class="fnv">PINNED</span>=pinnedFlag)
    <span class="fstd">if</span> (istat /= 0) <span class="fstd">then</span>
        <span class="fstd">write</span>(*,*) <span class="fstring">'Allocation of a failed'</span>
        stop
    <span class="fstd">else</span>
        <span class="fstd">if</span> (<span class="flog">.not.</span> pinnedFlag) <span class="fstd">write</span>(*,*) <span class="fstring">'Pinned allocation failed'</span>
    <span class="fstd">end</span> <span class="fstd">if</span>

    <span class="fcomment">! create events and streams</span>
    istat = <span class="fnv">cudaEventCreate</span>(startEvent)
    istat = <span class="fnv">cudaEventCreate</span>(stopEvent)
    istat = <span class="fnv">cudaEventCreate</span>(dummyEvent)
    <span class="fstd">do</span> i = 1, nStreams
        istat = <span class="fnv">cudaStreamCreate</span>(stream(i))
    <span class="fstd">end</span> <span class="fstd">do</span>

    <span class="fcomment">! baseline case - sequential transfer and execute</span>
    a = 0
    istat = <span class="fnv">cudaEventRecord</span>(startEvent, 0)
    a_d = a
    <span class="fstd">call</span> kernel <span class="fnv">&lt;&lt;&lt;n/blockSize, blockSize &gt;&gt;&gt;</span>(a_d, 0)
    a = a_d
    istat = <span class="fnv">cudaEventRecord</span>(stopEvent , 0)
    istat = <span class="fnv">cudaEventSynchronize</span>(stopEvent)
    istat = <span class="fnv">cudaEventElapsedTime</span>(time, startEvent, stopEvent)
    <span class="fstd">write</span>(*,*) <span class="fstring">'Time for sequential '</span>, <span class="fstring"><span class="fstring"><span class="fstring"><span class="fstring">'transfer and execute (ms): '</span></span></span></span>, time
    <span class="fstd">write</span>(*,*) <span class="fstring"><span class="fstring"><span class="fstring"><span class="fstring">' max error: '</span></span></span></span>, <span class="fintrinsic">maxval</span>(<span class="fintrinsic">abs</span>(a-1.0))

    <span class="fcomment">! asynchronous version 1: loop over {copy , kernel , copy}</span>
    a = 0
    istat = <span class="fnv">cudaEventRecord</span>(startEvent, 0)
    <span class="fstd">do</span> i = 1, nStreams
        offset = (i-1)* streamSize
        istat = <span class="fnv">cudaMemcpyAsync</span>(a_d(offset+1), a(offset+1), streamSize, stream(i))
        <span class="fstd">call</span> kernel <span class="fnv"><span class="fnv"><span class="fnv">&lt;&lt;&lt;streamSize/blockSize, blockSize, 0, stream(i)&gt;&gt;&gt;</span></span></span>(a_d ,offset)
        istat = <span class="fnv">cudaMemcpyAsync</span>(a(offset+1), a_d(offset+1), streamSize, stream(i))
    <span class="fstd">end</span> <span class="fstd">do</span>
    istat = <span class="fnv">cudaEventRecord</span>(stopEvent, 0)
    istat = <span class="fnv">cudaEventSynchronize</span>(stopEvent)
    istat = <span class="fnv">cudaEventElapsedTime</span>(time , startEvent , stopEvent)
    <span class="fstd">write</span>(*,*) <span class="fstring">'Time for asynchronous V1 '</span>, <span class="fstring"><span class="fstring"><span class="fstring"><span class="fstring">'transfer and execute (ms): '</span></span></span></span>, time
    <span class="fstd">write</span>(*,*) <span class="fstring"><span class="fstring"><span class="fstring"><span class="fstring">' max error: '</span></span></span></span>, <span class="fintrinsic">maxval</span>(<span class="fintrinsic">abs</span>(a-1.))

    <span class="fcomment">! asynchronous version 2:</span>
    <span class="fcomment"><span class="fcomment">! loop over copy</span> , loop over kernel , loop over copy</span>
    a = 0
    istat = <span class="fnv">cudaEventRecord</span>(startEvent, 0)
    <span class="fstd">do</span> i = 1, nStreams
        offset = (i-1)*streamSize
        istat = <span class="fnv">cudaMemcpyAsync</span>(a_d(offset+1), a(offset+1), streamSize, stream(i))
    <span class="fstd">end</span> <span class="fstd">do</span>
    <span class="fstd">do</span> i = 1, nStreams
        offset = (i-1)*streamSize
        <span class="fstd">call</span> kernel <span class="fnv"><span class="fnv"><span class="fnv">&lt;&lt;&lt;streamSize/blockSize, blockSize, 0, stream(i)&gt;&gt;&gt;</span></span></span>(a_d ,offset)
    <span class="fstd">end</span> <span class="fstd">do</span>
    <span class="fstd">do</span> i = 1, nStreams
        offset = (i-1)*streamSize
        istat = cudaMemcpyAsync (a(offset +1), a_d(offset+1), streamSize, stream(i))
    <span class="fstd">end</span> <span class="fstd">do</span>
    istat = <span class="fnv">cudaEventRecord</span>(stopEvent, 0)
    istat = <span class="fnv">cudaEventSynchronize</span>(stopEvent)
    istat = <span class="fnv">cudaEventElapsedTime</span>(time , startEvent , stopEvent)
    <span class="fstd">write</span>(*,*) <span class="fstring">'Time for asynchronous V2 '</span>, <span class="fstring"><span class="fstring"><span class="fstring"><span class="fstring">'transfer and execute (ms): '</span></span></span></span>, time
    <span class="fstd">write</span>(*,*) <span class="fstring"><span class="fstring"><span class="fstring"><span class="fstring">' max error: '</span></span></span></span>, <span class="fintrinsic">maxval</span>(<span class="fintrinsic">abs</span>(a-1.))

    <span class="fcomment">! asynchronous version 3:</span>
    <span class="fcomment"><span class="fcomment">! loop over copy</span> , loop over {kernel, event},</span>
    <span class="fcomment">! loop over copy</span>
    a = 0
    istat = <span class="fnv">cudaEventRecord</span>(startEvent ,0)
    <span class="fstd">do</span> i = 1, nStreams
        offset = (i-1)* streamSize
        istat = <span class="fnv">cudaMemcpyAsync</span>(a_d(offset+1), a(offset+1), streamSize ,stream(i))
    <span class="fstd">end</span> <span class="fstd">do</span>
    <span class="fstd">do</span> i = 1, nStreams
        offset = (i-1)* streamSize
        <span class="fstd">call</span> kernel <span class="fnv"><span class="fnv"><span class="fnv">&lt;&lt;&lt;streamSize/blockSize, blockSize, 0, stream(i)&gt;&gt;&gt;</span></span></span>(a_d ,offset)
        istat = <span class="fnv">cudaEventRecord</span>(dummyEvent, stream(i))
    <span class="fstd">end</span> <span class="fstd">do</span>
    <span class="fstd">do</span> i = 1, nStreams
        offset = (i-1)* streamSize
        istat = <span class="fnv">cudaMemcpyAsync</span>(a(offset+1), a_d(offset+1), streamSize, stream(i))
    <span class="fstd">end</span> <span class="fstd">do</span>
    istat = <span class="fnv">cudaEventRecord</span>(stopEvent, 0)
    istat = <span class="fnv">cudaEventSynchronize</span>(stopEvent)
    istat = <span class="fnv">cudaEventElapsedTime</span>(time, startEvent, stopEvent)
    <span class="fstd">write</span>(*,*) <span class="fstring">'Time for asynchronous V3 '</span>, <span class="fstring"><span class="fstring"><span class="fstring"><span class="fstring">'transfer and execute (ms): '</span></span></span></span>, time
    <span class="fstd">write</span>(*,*) <span class="fstring"><span class="fstring"><span class="fstring"><span class="fstring">' max error: '</span></span></span></span>, <span class="fintrinsic">maxval</span>(<span class="fintrinsic">abs</span>(a-1.))

    <span class="fcomment">! cleanup</span>
    istat = <span class="fnv">cudaEventDestroy</span>(startEvent)
    istat = <span class="fnv">cudaEventDestroy</span>(stopEvent)
    istat = <span class="fnv">cudaEventDestroy</span>(dummyEvent)
    <span class="fstd">do</span> i = 1, nStreams
        istat = <span class="fnv">cudaStreamDestroy</span>(stream(i))
    <span class="fstd">end</span> <span class="fstd">do</span>
    <span class="fstd">deallocate</span>(a)

<span class="fstd">end</span> <span class="fstd">program</span> testAsync</code></pre>
        </div>

        <hr>
        <div class="footer">
            <p>Content first published: 25 Jan 2023 &nbsp;&nbsp;&nbsp; Content last modified: 25 Jan 2023</p>
            <p><a href="mailto:edward_yang_125@hotmail.com"><img src="/static/logos/email.png"> Email</a>&nbsp;&nbsp;&nbsp; <a href="https://www.github.com/edoyango"><img src="/static/logos/github.png"> Github</a>&nbsp;&nbsp;&nbsp; <a href="https://www.linkedin.com/in/edward-yang-a0a9941b1"><img src="/static/logos/linkedin.png"> LinkedIn</a></p>
        </div>
        
    </body>
</html>
