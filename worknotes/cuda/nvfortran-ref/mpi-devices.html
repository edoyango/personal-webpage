<!doctype html>
<html>
    <head>
        <title>Ed Space - Worknotes - CUDA</title>
        <link href="/css/general.css" rel="stylesheet">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
        <link href="/css/fortran-syntax.css" rel="stylesheet">
    </head>
    <body>
    
        <!-- Top navigation bar -->
        <nav class="navbar navbar-default sticky-top">
            <div class="container-fluid">
                <ul class="nav navbar-nav">
                    <li class="nav-item"><a href="/index.html">Home</a></li>
                    <li class="nav-item"><a href="/about.html">About Me</a></li>
                    <li class="nav-item active"><a href="/worknotes/index.html">Work Notes</a></li>
                    <li class="nav-item"><a href="/trips/index.html">Trips</a></li>
                    <li class="nav-item"><a href="/apps/index.html">Web Tools</a></li>
                  </ul>
            </div>
        </nav>
        
        <div class="normalwidth">
            <h1>mpiDevices</h1>
            <p>Code to get started with using MPI with nvfortran. All it does is check compute mode:</p>
            <ul>
                <li>default (0): multiple host threads can use a single GPU</li>
                <li>exclusive (1): one host thread can use a single GPU at a time</li>
                <li>prohibited (2): No host threads can use the GPU</li>
                <li>exclusive process (3): Single contect cna be created by a single process, but that process can be current to all threads of that process.</li>
            </ul>
            <p><a href="samples/mpiDevices.cuf" download="mpiDevices.cuf">Download link</a></p>
            <pre><code><span class="fstd">program</span> mpiDevices

    <span class="fstd">use</span> <span class="fnv">cudafor</span>
    <span class="fstd">use</span> mpi

    <span class="fstd">implicit</span> <span class="fstd">none</span>
    <span class="fcomment">! global array size</span>
    <span class="fstd">integer</span>, <span class="fstd">parameter</span>:: n = 1024*1024
    <span class="fcomment">! MPI variables</span>
    <span class="fstd">integer</span>:: procid, numprocs, ierr
    <span class="fcomment">! device</span>
    <span class="fstd">type</span>(<span class="fnv">cudaDeviceProp</span>):: prop
    <span class="fstd">integer</span>(<span class="fnv">int_ptr_kind</span>()):: freeB, totalB, freeA, totalA
    <span class="fstd">real</span>, <span class="fnv">device</span>, <span class="fstd">allocatable</span>:: d(:)
    <span class="fstd">integer</span>:: i, istat, devid

    <span class="fcomment">! MPI initialization</span>
    <span class="fstd">call</span> <span class="fmpi">MPI_INIT</span>(ierr)
    <span class="fstd">call</span> <span class="fmpi">MPI_COMM_RANK</span>(<span class="fmpi">MPI_COMM_WORLD</span>, procid, ierr)
    <span class="fstd">call</span> <span class="fmpi">MPI_COMM_SIZE</span>(<span class="fmpi">MPI_COMM_WORLD</span>, numprocs, ierr)

    <span class="fcomment">! print compute mode for device</span>
    istat = <span class="fnv">cudaGetDevice</span>(devid)
    istat = <span class="fnv">cudaGetDeviceProperties</span>(prop, devid)

    <span class="fstd">do</span> i = 1, numprocs
        <span class="fstd">call</span> <span class="fmpi">MPI_BARRIER</span>(<span class="fmpi">MPI_COMM_WORLD</span>, ierr)
        <span class="fstd">if</span> (procid == i) <span class="fstd">write</span>(*,<span class="fstring">"(<span class="fstring">'['</span>,i0,<span class="fstring">'] using device: '</span>, i0, <span class="fstring">' in compute mode: '</span>, i0)"</span>) &
            procid, devid, prop%computeMode
    <span class="fstd">end</span> <span class="fstd">do</span>

    <span class="fcomment">! get memory use before large allocations</span>
    <span class="fstd">call</span> <span class="fmpi">MPI_BARRIER</span>(<span class="fmpi">MPI_COMM_WORLD</span>, ierr)
    istat = <span class="fnv">cudaMemGetInfo</span>(freeB, totalB)

    <span class="fcomment">! now allocate arrays, one rank at a time</span>
    <span class="fstd">do</span> j = 0, numprocs-1

        <span class="fcomment">! allocate on device associated with rank j</span>
        <span class="fstd">call</span> <span class="fmpi">MPI_BARRIER</span>(<span class="fmpi">MPI_COMM_WORLD</span>, ierr)
        <span class="fstd">if</span> (myrank == j) <span class="fstd">allocate</span>(d(n))

        <span class="fcomment">! Get free memory after allocation</span>
        <span class="fstd">call</span> <span class="fmpi">MPI_BARRIER</span>(<span class="fmpi">MPI_COMM_WORLD</span>, ierr)
        istat = <span class="fnv">cudaMemGetInfo</span>(freeA, totalA)

        <span class="fstd">write</span>(*,<span class="fstring">"(<span class="fstring">' ['</span>,i0,<span class="fstring">'] after allocation on rank: '</span>, i0, <span class="fstring">', device arrays allocated: '</span>, i0)"</span>) &
            procid, devid, (freeB-freeA)/n/4

    <span class="fstd">end</span> <span class="fstd">do</span>

    <span class="fstd">deallocate</span>(d)

    <span class="fstd">call</span> <span class="fmpi">MPI_FINALIZE</span>(ierr)

<span class="fstd">end</span> <span class="fstd">program</span> mpiDevices</code></pre>
        </div>

        <hr>
        <div class="footer">
            <p>Content first published: 26 Jan 2023 &nbsp;&nbsp;&nbsp; Content last modified: 26 Jan 2023</p>
            <p><a href="mailto:edward_yang_125@hotmail.com"><img src="/static/logos/email.png"> Email</a>&nbsp;&nbsp;&nbsp; <a href="https://www.github.com/edoyango"><img src="/static/logos/github.png"> Github</a>&nbsp;&nbsp;&nbsp; <a href="https://www.linkedin.com/in/edward-yang-a0a9941b1"><img src="/static/logos/linkedin.png"> LinkedIn</a></p>
        </div>
        
    </body>
</html>
