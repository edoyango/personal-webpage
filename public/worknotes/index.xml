<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Landing on Ed&#39;s Space - Notes</title>
    <link>/worknotes/</link>
    <description>Recent content in Landing on Ed&#39;s Space - Notes</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="/worknotes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Basic Comparison of C&#43;&#43; vs Fortran</title>
      <link>/worknotes/docs/f-cpp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/f-cpp/</guid>
      <description>A Basic Comparison of C++ vs Fortran # I&amp;rsquo;m an avid user of Fortran and this is pretty well known in my team. I&amp;rsquo;m not particularly evangalistic about using Fortran, but I do feel it has its place in modern programming, despite the fact that it&amp;rsquo;s one of the oldest programming languages out there. This has kind of been echoed by other Fortran users. For example, in Matsuoka et al.</description>
    </item>
    <item>
      <title>Combining the Orange Pi 5 Pro and YOLOv5 for bird detection</title>
      <link>/worknotes/docs/pi/yolov5-orangepi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/pi/yolov5-orangepi/</guid>
      <description>Combining the Orange Pi 5 Pro and YOLOv5 for bird detection # I started this project to record and identify the birds that we saw on our verandah throughout the day, as well as an excuse to learn about object detection. I aimed at using YOLO mainly because the ultralytics package is very easy to use and the performance-accuracy tradeoff was good.&#xA;When I first got started, it was quite easy to play with ultralytics and its latest iterations of YOLO on my laptop, but my main constraint was that I didn&amp;rsquo;t have any machines that were a good (and affordable) candidate to process the videos.</description>
    </item>
    <item>
      <title>CUDA &#43; NVHPC on WSL</title>
      <link>/worknotes/docs/cuda/cuda-wsl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/cuda-wsl/</guid>
      <description>Installing CUDA + NVHPC on WSL # This page describes the steps to setup CUDA and NVHPC within the WSL2 container (Windows 10) - avoiding the need for dual-boot or a separate Linux PC. Note that WSL2 must not have been installed when beginning these steps.&#xA;Install the latest Windows CUDA graphics driver Install WSL2 open PowerShell as administrator Make sure to update WSL kernel to latest version wsl &amp;ndash;update if accidentally rolled back, follow the instructions here then wsl &amp;ndash;update again check which Linux flavours are available with wsl &amp;ndash;list &amp;ndash;online install the desired flavour by wsl &amp;ndash;install -d start WSL with wsl, or opening the WSL application from the Windows search bar sudo apt update &amp;amp;&amp;amp; sudo apt upgrade -y Close and restart WSL sudo apt update &amp;amp;&amp;amp; sudo apt upgrade -y &amp;amp;&amp;amp; sudo apt autoremove -y Install CUDA for WSL check which CUDA version is compatible with the desired version NVHPC kit here select the correct CUDA version here Select the right setup: Linux -&amp;gt; x86_64 -&amp;gt; WSL-Ubuntu -&amp;gt; 2.</description>
    </item>
    <item>
      <title>deviceQuery</title>
      <link>/worknotes/docs/cuda/reference-codes/deviceQuery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/deviceQuery/</guid>
      <description>deviceQuery # Description # Function to query the properties of the NVIDIA GPUs detected on the system.&#xA;Code (C++) # #include &amp;lt;stdio.h&amp;gt; int main() { int nDevices; cudaGetDeviceCount(&amp;amp;nDevices); if (nDevices == 0) { printf(&amp;#34;No CUDA devices found\n&amp;#34;); } else if (nDevices == 1) { printf(&amp;#34;One CUDA device found\n&amp;#34;); } else { printf(&amp;#34;%d CUDA devices found\n&amp;#34;, nDevices); } // Loop over devices and print properties cudaDeviceProp prop; for (int i = 0; i &amp;lt; nDevices; ++i) { printf(&amp;#34;Device Number: %d\n&amp;#34;, i); cudaGetDeviceProperties(&amp;amp;prop, i); // General device info printf(&amp;#34; Device Name: %s\n&amp;#34;); printf(&amp;#34; Compute Capability: %d.</description>
    </item>
    <item>
      <title>Direct pair search</title>
      <link>/worknotes/docs/useful/fixed-cutoff-direct-pair-search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/useful/fixed-cutoff-direct-pair-search/</guid>
      <description>Point-pairs search with fixed cutoff distance (direct) # Description # I work with point pair searches through particle-based simulations (mostly SPH and DEM). The algorithm here is the most basic way to perform a pair search. It is O(N2) time, so is not useful for any practical applications.&#xA;I use it frequently to server as a reference when investigating other ways to search for pairs. It&amp;rsquo;s simple to code, so is harder to introduce conceptual and coding errors.</description>
    </item>
    <item>
      <title>Fixed-Radius Neighbour Search Using Thrust</title>
      <link>/worknotes/docs/cuda/frn-thrust/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/frn-thrust/</guid>
      <description>Fixed-Radius Neighbour Search Using Thrust # This page describes how one might implement the pair-finding algorithm described by Simon Green (2010). Many implementations of Simon&amp;rsquo;s work exists, such as the FRNN Python package.&#xA;Why Thrust? # I would argue that there is a fairly strong incentive to make GPU-accelerated codes portable across different hardware platforms. At the time of writing, NVIDIA GPUs are becoming more challenging to get a hold of, and are more expensive relative to their competition (i.</description>
    </item>
    <item>
      <title>Motivation</title>
      <link>/worknotes/docs/manual-vectorization/motivation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/manual-vectorization/motivation/</guid>
      <description>SPH is a continuum particle method that is often used for simulations. Typically, the most time consuming part of codes that aim to perform SPH simulations, is finding the pairs of SPH particles that are within a fixed-cutoff of each other (the pair-search step from herein), and calculating the contribution to particles&amp;rsquo; motion, due to its corresponding pair (the force calculation sweep step from herein). These steps can be combined together when organising the code, but it&amp;rsquo;s useful to seperate them when needing to re-use the pair list.</description>
    </item>
    <item>
      <title>OpenFOAM cavity case to CFDEM</title>
      <link>/worknotes/docs/cfdem/cavitycfdem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cfdem/cavitycfdem/</guid>
      <description>Converting the OpenFOAM cavity example for CFDEM # The cavity case is a good one to start with as it forms the beginning of the official OpenFOAM tutorials. These steps assume we&amp;rsquo;re using the PUBLIC version of CFDEM which couples LIGGGHTS-PUBLIC 3.8.0 and OpenFOAM-5.x. It also assumes that your environment variables have already been setup as per the CFDEM insallation instructions.&#xA;Getting the case files # The lid-driven cavity flow example case comes with the OpenFOAM source code.</description>
    </item>
    <item>
      <title>Reducing Jetson Nano OS for Server</title>
      <link>/worknotes/docs/pi/trim-jetsonnano/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/pi/trim-jetsonnano/</guid>
      <description>Reducing Jetson Nano OS for Server # The Nvidia Jetson Nano is a Single Board Computer (SBC) with a scaled-down Nvidia GPU (Tegra X1). I have the 2GB version, the smallest available. The OS that Nvidia forces you to use comes with a full-blown desktop environment, which chews through the 2GB of RAM pretty easily - not leaving as much room as I&amp;rsquo;d like for other things.&#xA;Consequently, this page is to document the steps to trim down the OS to save disk space and RAM - adding to steps documented elsewhere.</description>
    </item>
    <item>
      <title>Setting Up Public Webserver on Raspberry Pi</title>
      <link>/worknotes/docs/pi/webserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/pi/webserver/</guid>
      <description>Setting Up Public Webserver on Raspberry Pi # The instructions here assumes you&amp;rsquo;re using Raspberry Pi Lite as the OS on the Raspberry Pi. Other OS&amp;rsquo; are largely similar though. The main difference will be the packages and package managers, and the firewall tool.&#xA;Setup the pi to host the server&#xA;Flash disk with raspberry pi lite. Insert disk into pi and power them on. (follow the instructions here up to step 4) Setup router to forward http/https/ssh requests to the Raspberry Pi</description>
    </item>
    <item>
      <title>Setting Up Raspberry Pi Slurm Cluster</title>
      <link>/worknotes/docs/pi/slurm-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/pi/slurm-cluster/</guid>
      <description>Setting Up Raspberry Pi Slurm Cluster # Flash disk(s) with raspberry pi lite. Insert disk(s) into pi(s) and power them on.&#xA;Run sudo raspi-config&#xA;update raspi-config change hostname setup ssh change password for pi user set wlan locale set timezone setup login-less ssh between nodes.&#xA;create key on one of the nodes&#xA;sudo ssh-keygen # save it somewhere central like in /etc/ssh configure ssh to use the newly created key by editing /etc/ssh/ssh_config and adding</description>
    </item>
    <item>
      <title>Snappy Hex Mesh Basics</title>
      <link>/worknotes/docs/cfdem/snappyhexmesh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cfdem/snappyhexmesh/</guid>
      <description>Snappy Hex Mesh Basics # This is a summary of meshing in OpenFOAM using the snappyHexMesh tool. I&amp;rsquo;m writing this in detail because I couldn&amp;rsquo;t find any comprehensive tutorial that is beginner friendly. The ones I could find were like as if they were picking up from where someone else left off. Consequently, this tool is a beginner guide and aims only to recommend easy-to-pickup tools, rather than the most fully featured tools.</description>
    </item>
    <item>
      <title>Cell list pair search</title>
      <link>/worknotes/docs/useful/fixed-cutoff-cell-lists-pair-search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/useful/fixed-cutoff-cell-lists-pair-search/</guid>
      <description>Point-pairs search with fixed cutoff distance (using cell-lists) # Description # This is an improvement to the direct search algorithm for searching for pairs of points within a cutoff distance. It uses a grid of cells whose side-length is equal to the specified cutoff distance.&#xA;This is beneficial to performance since for any given point, all its neighbours are guaranteed to be within its own cell, or in adjacent cells.</description>
    </item>
    <item>
      <title>Coarray Fortran Things</title>
      <link>/worknotes/docs/caf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/caf/</guid>
      <description>Coarray Fortran Things # ignoring the basics. Best tutorial to start with is this one. Basic, but couldn&amp;rsquo;t find very beginner friendly ones.&#xA;Install # OpenCoarrays is a library usable with gfortran and uses MPI 1-sided comms as the to perform the communications. install via linuxbrew or spack.&#xA;Intel compilers don&amp;rsquo;t rely on external libraries and should be ready to use coarrays with intel-MPI. It only requires compilation with -coarray option.</description>
    </item>
    <item>
      <title>precision_m</title>
      <link>/worknotes/docs/cuda/reference-codes/precision_m/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/precision_m/</guid>
      <description>precision_m # Description # Module containing static types for single and double precision. Note that nvfortran has the options that can be used to demote/promote the precision of real variable/parameter declarations e.g. -r4 asks the compiler to interpret real declarations as real(4), -r8 interprets real as real(8), and -M[no]r8 will promote real declarations to double precision. nvc, nvcc, and nvc++ don&amp;rsquo;t have an equivalent as far as I know.</description>
    </item>
    <item>
      <title>Speeding Up LIGGGHTS with Intel Compilers and Compiler Options</title>
      <link>/worknotes/docs/cfdem/liggghts-intel-comp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cfdem/liggghts-intel-comp/</guid>
      <description>Speeding Up LIGGGHTS with Intel Compilers and Compiler Options # This page looks at using basic optimization options and Intel OneAPI Compilers (for x86 CPU architectures) to reduce the run-time of LIGGGHTS.&#xA;The page will first show the difference in performance of the Intel compilers compared to the GNU compilers and also looks at different compiler options. After hopefully convincing you of why you should use the intel compilers, the page then goes on to explain how to build LIGGGHTS with the Intel compilers.</description>
    </item>
    <item>
      <title>Vectorizing Array Addition</title>
      <link>/worknotes/docs/manual-vectorization/addition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/manual-vectorization/addition/</guid>
      <description>Vectorizing Array Addition # A common place to start, is to manually vectorize the addition of two arrays, and storing the result in a third array (from herein, ABC addition):&#xA;void abc_base_sgl(const int nelem, float* a, float* b, float*) { for (int i = 0; i &amp;lt; nelem; ++i) { c[i] = a[i] + b[i]; } } In the benchmarking code, the arrays are created with new.&#xA;Doing the Vectorization # Vectorizing the ABC function with 128-bit vectors (SSE instructions) # This function, manually vectorized, looks like:</description>
    </item>
    <item>
      <title>Cell list pair search - reducing search space</title>
      <link>/worknotes/docs/useful/fixed-cutoff-cell-lists-pair-searchhalf-search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/useful/fixed-cutoff-cell-lists-pair-searchhalf-search/</guid>
      <description>Cell-lists point-pairs search with fixed cutoff - reducing search space # Description # The naive cell-list pair search, is a good start. But a drawback of that implementation is that it searches through all adjacent cells, and uses an if statement to avoid duplicate checks. However, looping over all adjacent cells still takes time, and the if statement can be expensive by introducing branching.&#xA;However, to eliminate duplicate checks, we only need to compare particles in the current cell with particles in half of the adjacent cells.</description>
    </item>
    <item>
      <title>Error Handling</title>
      <link>/worknotes/docs/cuda/reference-codes/error/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/error/</guid>
      <description> Error Handling # Description # Basic functionality to check errors in CUDA functions and kernel subroutines.&#xA;The C++ code is very similar to the Fortran code, so I&amp;rsquo;m not including it.&#xA;Code (Fortran) # ! the cuda GetErrorString function can be used to obtain error messages from error codes ierr = cudaGetDeviceCount(nDevices) if (ierr/= cudaSuccess) write (*,*) cudaGetErrorString(ierr) ! kernel errors are checked using cudaGetLastError call increment &amp;lt;&amp;lt;&amp;lt;1,n&amp;gt;&amp;gt;&amp;gt;(a_d , b) ierrSync = cudaGetLastError() ierrAsync = cudaDeviceSynchronize() if (ierrSync /= cudaSuccess) write(*,*) &amp;#39;Sync kernel error&amp;#39;, cudaGetErrorString(ierrSync) if (ierrAsync /= cudaSuccess) write(*,*) &amp;#39;Async kernel error:&amp;#39;, cudaGetErrorString(cudaGetLastError()) </description>
    </item>
    <item>
      <title>Vector Sum Reduction</title>
      <link>/worknotes/docs/manual-vectorization/sumreduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/manual-vectorization/sumreduce/</guid>
      <description>Vector Sum Reduction # We now we can add two SIMD vectors together element-by-element. However, I&amp;rsquo;m sure you can imagine scenarios where you might want to add all the elements in a vector together i.e., a sum-reduction. This page will explain how to do this with 128-bit and 256-bit vectors the approach is different for each vector width. I would show how to do this with 512-bit vectors, but I don&amp;rsquo;t have a CPU with AVX512 instructions handy.</description>
    </item>
    <item>
      <title>Faster Vector Sum Reduction</title>
      <link>/worknotes/docs/manual-vectorization/faster-sumreduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/manual-vectorization/faster-sumreduce/</guid>
      <description>Faster Vector Sum Reduce # In my page introducing vectorized reductions, I show how to use horizontal adds (hadd) functions to perform the reduction. However, this doesn&amp;rsquo;t produce optimal assembly, so isn&amp;rsquo;t the fastest way to do things.&#xA;This page will demonstrate a faster version to do each of the examples shown previously.&#xA;Single precision, SSE instructions # This section will show a faster sum-reduce example which makes use of 128-bit single precision vectors and SSE instructions.</description>
    </item>
    <item>
      <title>grid dimension hashing</title>
      <link>/worknotes/docs/useful/grid-rows-spatial-hashing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/useful/grid-rows-spatial-hashing/</guid>
      <description>Hashing grid cell indices based on grid dimensions # Description # This strategy is based on this NVIDIA article. The idea being that instead of storing indices of particles in a grid data structure, you can convert these 3-valued indices to single hashes. These hashes can then be used to sort the particle data so that the particle data is ordered based on their grid cell hash index. This is beneficial for GPUs, which is why it&amp;rsquo;s mentioned in the above article, but is also useful for CPUs as it iterating over the particle pairs more cache-friendly.</description>
    </item>
    <item>
      <title>limitingFactor</title>
      <link>/worknotes/docs/cuda/reference-codes/limitingFactor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/limitingFactor/</guid>
      <description>limitingFactor # Description # Code to test whether computation or memory transfer is the bottleneck. Compiled program intended to be run with nvprof.&#xA;The book demonstrates the effect of compiling with -Mcuda=fastmath, which shows a significant speedup in the &amp;ldquo;base&amp;rdquo; and &amp;ldquo;math&amp;rdquo; kernels (note they use very old C2050 and K20 GPUs).&#xA;Code (C++) # #include &amp;lt;stdio.h&amp;gt; __global__ void base(float *a, float *b) { int i = blockIdx.x * blockDim.</description>
    </item>
    <item>
      <title>Vectorizing A Simple Pair Sweep</title>
      <link>/worknotes/docs/manual-vectorization/sweep/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/manual-vectorization/sweep/</guid>
      <description>Vectorizing A Simple Pair Sweep # One way to perform the force calculation sweep in SPH, is to use a double loop like:&#xA;for (int i = 0; i &amp;lt; nelem-1; ++i) { for (int j = i+1; j &amp;lt; nelem; ++j) { // compare particle i and j // calculate forces } } This is generally not a preferred way to do the force calculations, as the number of operations in this algorithm scales with the square of the number of particles.</description>
    </item>
    <item>
      <title>Vectorizing Cell-based Pair Search</title>
      <link>/worknotes/docs/manual-vectorization/cll-avx512/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/manual-vectorization/cll-avx512/</guid>
      <description>Vectorizing Cell-based Pair Search # To calculate the acceleration of SPH particles, one must first find the pairs. In the case where incompressible or weakly-compressible assumptions are used, and where particles&amp;rsquo; kernel radius is fixed, the cell-linked list strategy is often employed. The basis of the algorithm is that described on one of my other pages written in Fortran. Below is the C++ version of the main sweep code.</description>
    </item>
    <item>
      <title>Cell lists pair search without &#34;if&#34;</title>
      <link>/worknotes/docs/useful/fixed-cutoff-pair-search-noifs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/useful/fixed-cutoff-pair-search-noifs/</guid>
      <description>Cell lists pair search without if # Description # So far, the speedup demonstrated from the fixed-cutoff cell-lists pair search algorithm is pretty great. One last change we can make to improve things, is to remove any if statements when searching adjacent cells. if&amp;rsquo;s are undesirable because they introduce branching and harms performance. I&amp;rsquo;ve found that it can be beneficial to remove if statements, even if it means a bit more computation/assignments are performed.</description>
    </item>
    <item>
      <title>peakBandwidth</title>
      <link>/worknotes/docs/cuda/reference-codes/peakBandwidth/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/peakBandwidth/</guid>
      <description>peakBandwidth # Description # Code to obtain theoretical peak memory bandwidth of GPUs on the system.&#xA;Effective bandwidth can be obtained with&#xA;\[bw_e = (r_B &amp;#43; w_B)/(t\cdot10^9)\] where \(bw_e\) is the effective bandiwdth, \(r_B\) is the number of Bytes read, \(w_B\) is the number of Bytes written, and \(t\) is elapsed wall time in seconds.&#xA;The wall time of the simple memory kernel written in the limitingFactor code can be used.</description>
    </item>
    <item>
      <title>bandwidthTest</title>
      <link>/worknotes/docs/cuda/reference-codes/bandwidthTest/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/bandwidthTest/</guid>
      <description>bandwidthTest # Description # Demonstrates the increase of bandwidth when reading/writing from/to pinned host memory instead of normal &amp;ldquo;pageable&amp;rdquo; memory.&#xA;Code (C++) # #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;cstdlib&amp;gt; int main() { const int nElements = 4*1024*1024; size_t nbytes = nElements*sizeof(float); float *a_pageable, *b_pageable; float *a_pinned, *b_pinned; float *a_d; int ierr_a, ierr_b; cudaDeviceProp prop; cudaEvent_t startEvent, stopEvent; float time = 0.0; // pageable host memory a_pageable = (float*)malloc(nbytes); b_pageable = (float*)malloc(nbytes); // pinned host memory ierr_a = cudaMallocHost((void**)&amp;amp;a_pinned, nbytes); ierr_b = cudaMallocHost((void**)&amp;amp;b_pinned, nbytes); if (ierr_a !</description>
    </item>
    <item>
      <title>Z-curve hashing</title>
      <link>/worknotes/docs/useful/z-curve-spatial-hashing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/useful/z-curve-spatial-hashing/</guid>
      <description> Hashing grid cell indices based on Z-order curve # Description # Code (Fortran) # Coming soon... </description>
    </item>
    <item>
      <title>MemCpy</title>
      <link>/worknotes/docs/cuda/reference-codes/MemCpy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/MemCpy/</guid>
      <description>Memcpy # Description # Alternatives to array assignment to transfer data between host and device.&#xA;Calls to the Memcpy function may be beneifical as transfers by array assignment can be implicitly broken up into multiple transfers, slowing down the transfer.&#xA;cudaMemcpy is used heaps in the other C++ example codes, so I didn&amp;rsquo;t bother including a sample here.&#xA;Code (Fortran) # ! for contiguous data istat = cudaMemcpy(a_d , a_pageable , nElements) !</description>
    </item>
    <item>
      <title>testAsync</title>
      <link>/worknotes/docs/cuda/reference-codes/testAsync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/testAsync/</guid>
      <description>testAsync # Description # Program to demonstrate the run-time improvements when performing asynchronous memory transfers and execution.&#xA;In the book, they show three different batching approaches, which yield different results based on the GPU. On my NVIDIA 1650 (Turing architecture), I see V1 producing best results most of the time. On my workplace&amp;rsquo;s P100 (Keplar), A30 (Ampere), and A100 (Ampere) GPUs, the lowest run times are reliably obtained with V2 (but only about 1% difference).</description>
    </item>
    <item>
      <title>offsetNStride</title>
      <link>/worknotes/docs/cuda/reference-codes/offsetNStride/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/offsetNStride/</guid>
      <description>offsetNStride # Description # Demonstration of how coalesced access to GPU global memory i.e., accessing memory in strides of 16 (half-warp) or 32 (warp) can reduce the number of transactions made and reduce run times.&#xA;My NVIDIA 1650 behaves like K20 and C2050 used in the book, where 0-stride accesses are fastest, and everything else is worse (by only a little).&#xA;Code (C++) # #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;precision.h&amp;gt; __global__ void offset(userfp_t* a, int s) { int i = blockDim.</description>
    </item>
    <item>
      <title>strideTexture</title>
      <link>/worknotes/docs/cuda/reference-codes/strideTexture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/strideTexture/</guid>
      <description>strideTexture # Description # A demonstration showing how the use of textured memory pointers can improve strided global memory access.&#xA;I find that using textured memory pointers didn&amp;rsquo;t improve anything reliably on my NVIDIA 1650.&#xA;The deprecation is also why a C++ version is not provided here.&#xA;Code (Fortran) # module kernels_m real, texture, pointer:: aTex (:) contains attributes(global) subroutine stride(b, a, s) real:: b(*), a(*) integer, value:: s integer:: i, is i = blockDim%x*( blockIdx%x-1)+ threadIdx%x is = (blockDim%x*( blockIdx%x-1)+ threadIdx%x) * s b(i) = a(is)+1 end subroutine stride attributes(global) subroutine strideTex(b, s) real:: b(*) integer, value:: s integer:: i, is i = blockDim%x*( blockIdx%x-1)+ threadIdx%x is = (blockDim%x*( blockIdx%x-1)+ threadIdx%x) * s b(i) = aTex(is)+1 end subroutine strideTex end module kernels_m program strideTexture use cudafor use kernels_m implicit none integer, parameter:: nMB = 4 !</description>
    </item>
    <item>
      <title>sharedExample</title>
      <link>/worknotes/docs/cuda/reference-codes/sharedExample/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/sharedExample/</guid>
      <description>sharedExample # Description # A sample code to demonstrate how the compiler uses various types of memory. This information is availed when compiling with -Mcuda=ptxinfo.&#xA;Code (C++) # /* This code shows how dynamically and statically allocated shared memory are used to reverse a small array */ #include &amp;lt;stdio.h&amp;gt; __global__ void staticReverse(float* d, int n) { __shared__ float s[64]; int t = threadIdx.x, tr = n - t - 1; s[t] = d[t]; __syncthreads(); d[t] = s[tr]; } __global__ void dynamicReverse1(float* d, int n) { extern __shared__ float s[]; int t = threadIdx.</description>
    </item>
    <item>
      <title>checkP2PAccess</title>
      <link>/worknotes/docs/cuda/reference-codes/checkP2pAccess/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/checkP2pAccess/</guid>
      <description> checkP2pAccess # Description # Tool to check peer-to-peer connectivity between GPUs connected to the motherboard.&#xA;Code (C++) # #include &amp;lt;stdio.h&amp;gt; int main() { int nDevices; cudaGetDeviceCount(&amp;amp;nDevices); printf(&amp;#34;Number of CUDA-capable devices: %d\n&amp;#34;, nDevices); cudaDeviceProp prop; for (int i = 0; i &amp;lt; nDevices; ++i) { cudaGetDeviceProperties(&amp;amp;prop, i); printf(&amp;#34;Device %d: %s\n&amp;#34;, i, prop.name); } int p2pOK[nDevices][nDevices]; for (int i = 0; i &amp;lt; nDevices; ++i) { for (int j = i+1; j &amp;lt; nDevices; ++j) { cudaDeviceCanAccessPeer(&amp;amp;p2pOK[i][j], i, j); p2pOK[j][i] = p2pOK[i][j]; } } printf(&amp;#34;\n&amp;#34;); for (int i = 0; i &amp;lt; nDevices; ++i) { printf(&amp;#34; %3d&amp;#34;, i); } printf(&amp;#34;\n&amp;#34;); for (int j = 0; j &amp;lt; nDevices; ++j) { printf(&amp;#34;%3d&amp;#34;,j); for (int i = 0; i &amp;lt; nDevices; ++i) { if (i==j) { printf(&amp;#34; - &amp;#34;); } else if (p2pOK[i][j] == 1) { printf(&amp;#34; Y &amp;#34;); } else { printf(&amp;#34; &amp;#34;); } } printf(&amp;#34;\n&amp;#34;); } } Code (Fortran) # program checkP2pAccess use cudafor implicit none integer, allocatable:: p2pOK(:,:) integer:: nDevices, i, j, istat type (cudaDeviceProp):: prop istat = cudaGetDeviceCount(nDevices) write(*,&amp;#34;(&amp;#39;Number of CUDA -capable devices: &amp;#39;, i0,/)&amp;#34;) nDevices do i = 0, nDevices -1 istat = cudaGetDeviceProperties(prop, i) write(*,&amp;#34;(&amp;#39;Device &amp;#39;, i0, &amp;#39;: &amp;#39;, a)&amp;#34;) i, trim(prop%name) end do write(*,*) allocate(p2pOK (0: nDevices -1, 0: nDevices -1)) p2pOK = 0 do j = 0, nDevices -1 do i = j+1, nDevices -1 istat = cudaDeviceCanAccessPeer(p2pOK(i,j), i, j) p2pOK(j,i) = p2pOK(i,j) end do end do do i = 0, nDevices -1 write(*,&amp;#34;(3x,i3)&amp;#34;, advance=&amp;#39;no&amp;#39;) i end do write(*,*) do j = 0, nDevices -1 write(*,&amp;#34;(i3)&amp;#34;, advance=&amp;#39;no&amp;#39;) j do i = 0, nDevices -1 if (i == j) then write(*,&amp;#34;(2x,&amp;#39;-&amp;#39;,3x)&amp;#34;, advance=&amp;#39;no&amp;#39;) else if (p2pOK(i,j) == 1) then write(*,&amp;#34;(2x, &amp;#39;Y&amp;#39;,3x)&amp;#34;,advance=&amp;#39;no&amp;#39;) else write(*,&amp;#34;(6x)&amp;#34;,advance=&amp;#39;no&amp;#39;) end if end do write(*,*) end do end program checkP2pAccess </description>
    </item>
    <item>
      <title>directTransfer</title>
      <link>/worknotes/docs/cuda/reference-codes/directTransfer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/directTransfer/</guid>
      <description>directTransfer # Description # Demonstration code showing the difference in data transfer rates when transfering directly between peer GPUs, versus without p2p transfer.&#xA;Running this on the GPUs at work, showed the transfer between GPUs with p2p disabled was maybe 5% slower, but this wasn&amp;rsquo;t produced reliably. The differnece was not nearly as dramatic as those in the book.&#xA;Code (C++) # To do... Code (Fortran) # program directTransfer use cudafor implicit none integer, parameter:: N = 4*1024*1024 real, pinned, allocatable:: a(:), b(:) real, device, allocatable:: a_d(:), b_d(:) !</description>
    </item>
    <item>
      <title>p2pBandwidth</title>
      <link>/worknotes/docs/cuda/reference-codes/p2pBandwidth/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/p2pBandwidth/</guid>
      <description>p2pBandwidth # Description # Program to measure the bandwidth of memory transfer between a GPUs on a multi-GPU host.&#xA;A useful technique shown here, is the use of derived types with device member arrays to manage each GPU&amp;rsquo;s instance of the device arrays.&#xA;Code (C++) # #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;cstdlib&amp;gt; struct distributedArray { float* a_d; }; __global__ void setVal(float* __restrict__ array, float val) { int i = threadIdx.x + blockIdx.</description>
    </item>
    <item>
      <title>mpiDevices</title>
      <link>/worknotes/docs/cuda/reference-codes/mpiDevices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/mpiDevices/</guid>
      <description>mpiDevices # Description # Code to get started with using MPI with nvfortran. All it does is check compute mode:&#xA;default (0): multiple host threads can use a single GPU exclusive (1): one host thread can use a single GPU at a time prohibited (2): No host threads can use the GPU exclusive process (3): Single contect cna be created by a single process, but that process can be current to all threads of that process.</description>
    </item>
    <item>
      <title>mpiDeviceUtil</title>
      <link>/worknotes/docs/cuda/reference-codes/mpiDeviceUtil/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/mpiDeviceUtil/</guid>
      <description>mpiDeviceUtil # Description # Basic module to assign MPI processes to unique GPUs. This is modified from the code in the book to use MPI_ALLGATHER, an explicit definition of the quicksort subroutine, and the use of hostnm Fortran intrinsic function instead of the MPI_GET_PROCESSOR_NAME.&#xA;Code (C++) # To do... Code (Fortran) # module mpiDeviceUtil contains subroutine assignDevice(procid, numprocs, dev) use mpi use cudafor implicit none integer:: numprocs, procid, dev character(len=100), allocatable:: hosts(:) character(len=100):: hostname integer:: namelength, color, i integer:: newComm, newProcid, ierr logical:: mpiInitialized !</description>
    </item>
    <item>
      <title>transposeMPI</title>
      <link>/worknotes/docs/cuda/reference-codes/transposeMPI/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/reference-codes/transposeMPI/</guid>
      <description>transposeMPI # Description # Code showing the use of tiling and shared memory in transposing a matrix. The book uses it as an example of the performance difference between CUDA aware MPI vs non-MPI transfers (transfers between GPUs via their respective host CPUs). The code will work on GPUs communicating across node boundaries.&#xA;Code (C++) # To do... Code (Fortran) # module transpose_m implicit none integer, parameter:: cudaTileDim = 32 integer, parameter:: blockRows = 8 contains attributes(global) subroutine cudaTranspose(odata, ldo, idata, ldi) real, intent(out):: odata(ldo, *) real, intent(in):: idata(ldi, *) integer, value, intent(in):: ldo, ldi real, shared:: tile(cudaTileDim+1, cudaTileDim) integer:: x, y, j x = (blockIdx%x-1) * cudaTileDim + threadIdx%x y = (blockIdx%y-1) * cudaTileDim + threadIdx%y do j = 0, cudaTileDim-1, blockRows tile(threadIdx%x, threadIdx%y+j) = idata(x, y+j) end do call syncthreads() x = (blockIdx%y-1) * cudaTileDim + threadIdx%x y = (blockIdx%x-1) * cudaTileDim + threadIdx%y do j = 0, cudaTileDim-1, blockRows odata(x, y+j) = tile(threadIdx%y+j, threadIdx%x) end do end subroutine end module transpose_m !</description>
    </item>
    <item>
      <title></title>
      <link>/worknotes/docs/cuda/hidden/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/worknotes/docs/cuda/hidden/</guid>
      <description>This page is hidden in menu # Quondam non pater est dignior ille Eurotas # Latent te facies # Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.&#xA;Pater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor Cum honorum Latona # O fallor in sustinui iussorum equidem.</description>
    </item>
  </channel>
</rss>
